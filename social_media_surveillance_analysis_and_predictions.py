# -*- coding: utf-8 -*-
"""Social Media Surveillance Analysis and Predictions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/amusakehindeabraham/social-media-surveillance-analysis-and-predictions.e071fd8c-9832-4095-9e7f-7912a7b98906.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250507/auto/storage/goog4_request%26X-Goog-Date%3D20250507T122646Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dae02a201bb08c87a1586dffc20831734bb7fb6bdce5c1713cb0b175cd157e2f118b2734ac924f1bfc8b96c7b7b3feaf471d4ebae5999264b35782208f6d05b7ea88e4e9b2e6e27b8dc55603adcdd1e22704a4094e41ee7c871dedb5b4f271b9cd718ebd0c60691e6af73169118a70f1d6507c7ef9936ad7df5cbba3bb64928c948311a6c1181e43e3c41e282ec4c7a45e8b361a61e0b7b885fc296bb9acffe89c51dfa24a8712a1833a239913a3926dc1c79f2e714a8ccecac3362c6077e7b011db5a08f4f855893ef89b3e4740965eda3ba3b70952157ac698bef969799e58e8c5725a647c6f50480b9146ee0c2af11ee7b59815d1660c4ca11e33b5f215ecc
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Loading the dataset
file_path = '/960scopus.csv'
df = pd.read_csv(file_path)
df.head()

"""### Data Overview

Let's take a closer look at the structure of our dataset and understand the types of data we are dealing with.
"""

# Displaying basic information about the dataset
df.info()

"""### Data Cleaning and Preprocessing

Before diving into the analysis, we need to clean and preprocess the data. This includes handling missing values, converting data types, and extracting relevant features.
"""

# Handling missing values
df = df.dropna(subset=['Title', 'Year', 'Cited by', 'Link', 'Abstract', 'Language of Original Document', 'Document Type', 'Publication Stage', 'Source', 'EID'])

# Converting data types
df['Year'] = df['Year'].astype(int)
df['Cited by'] = df['Cited by'].astype(int)

# Displaying the cleaned dataset
df.head()

"""### Exploratory Data Analysis (EDA)

Let's explore the dataset to uncover interesting patterns and trends. We'll start with some basic visualizations.
"""

# Distribution of publications over the years
plt.figure(figsize=(10, 6))
sns.countplot(x='Year', data=df, palette='viridis')
plt.title('Number of Publications Over the Years')
plt.xticks(rotation=45)
plt.show()

# Top 10 most cited papers
top_cited = df.nlargest(10, 'Cited by')
plt.figure(figsize=(10, 6))
sns.barplot(x='Cited by', y='Title', data=top_cited, palette='viridis')
plt.title('Top 10 Most Cited Papers')
plt.show()

"""### Correlation Analysis

Let's examine the correlation between numeric variables in the dataset.
"""

# Selecting numeric columns
numeric_df = df.select_dtypes(include=[np.number])

# Correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='viridis')
plt.title('Correlation Heatmap')
plt.show()

"""### Predictive Analysis

Given the data, it might be interesting to predict the number of citations a paper will receive based on its features. Let's build a simple linear regression model to predict the 'Cited by' count.
"""

import pickle
# Selecting features and target variable
X = df[['Year']]
y = df['Cited by']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Building the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)


# Making predictions
y_pred = model.predict(X_test)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mse, r2



"""### Future Analysis

There are numerous directions we could take for further analysis. For instance, we could explore the impact of different document types on citation counts, or analyze the effect of open access on the number of citations. What do you think would be useful to explore next?

### Conclusion

In this notebook, we explored the fascinating dataset on social media surveillance, performed some exploratory data analysis, and built a simple predictive model. The insights gained here are just the tip of the iceberg. If you found this notebook useful, please upvote it.

## Credits
This notebook was created with the help of [Devra AI data science assistant](https://devra.ai/ref/kaggle)
"""